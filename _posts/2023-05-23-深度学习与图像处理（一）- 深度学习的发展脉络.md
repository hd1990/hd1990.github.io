---
layout: post
title: 深度学习与图像处理（一）- 深度学习的发展脉络
date: 2023-05-23
description: "深度学习与图像处理（一）- 深度学习的发展脉络"
tag: 机器学习
---
## 深度学习的发展脉络

### 神经元模型与感知机

每个神经元可以与多达成百上千个其他神经元的突触进行连接，接收从各个突触传来的信号输入。神经元细胞体的膜电位是它所有突触产生的电位总和，当膜电位超过某一阈值时，神经元会被激活并产生一个脉冲信号。脉冲信号将通过突触传递给下一个神经元。

$\mathrm{M}-\mathrm{P}$ 神经元模型正是对生物神经元模型的一个抽象和简化, 如图1所示为 **M-P 神经元模型**的示意图。其中， $x_1, x_2, \cdots, x_n$ 表示输入信号, $w_1, w_2, \cdots, w_n$ 表示权重, ○表示神经元, $y$表示输出信号。当输入信号被传送给神经元时, 会被分别乘以固定的权重。神经元会计算所有被传送过来的信号总和, 当总和超过阈值 $b$ 时, 神经元被激活, 此时输出1;否则,输出0。上述过程可以使用公式表达如下:

$$
y= \begin{cases}0, & x_1 w_1+x_2 w_2+\cdots+x_n w_n \leqslant b \\ 1, & x_1 w_1+x_2 w_2+\cdots+x_n w_n>b\end{cases}
$$

![20230523092229](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230523092229.png)

图1 M-P 神经元模型示意图

可以看出, M-P 神经元模型的本质是进行**加权求和运算并根据阈值进行激活**的过程。

### 感知机

M-P 模型的权重和偏置是固定的，因此不具备学习的可能。1958 年由美国心理学家Rosenblatt 解提出了第一个具有学习能力的单层计算单元的神经网络，称之为感知机(perceptron)。

单层感知机的出现成为神经网络发展的基石。

如图2所示，输入x 的层可以叫作感知层，包含n个神经元，每个神经元接收一个输入信号，这n 个神经元仅将外部输入信息接收进来。用符号 X 表示这 n 个神经元的输入，则 $X = [x_1, x_2 ,\cdots, x_n]$。

处理层包含 m 个神经元，用符号W 表示，这m 个神经元可以处理感知层传输过来的信息，具有信息处理能力，并将处理后的信息进行输出。其中，符号W 表示进行数据处理时，感知层和处理层之间的连接权值，则$W = [w_1, w_2,  \cdots, w_m ]$，符号 O 表示经处理层处理后输出的信息，则$O = [o_1,o_2,\cdots,o_m]$。

用感知机的处理过程这种方式改写一下之前的M-P 神经元模型。

$$
o_{i}=\left\{\begin{array}{ll}
1, & b+W^{\mathrm{T}} X \leqslant 0 \\
0, & b+W^{\mathrm{T}} X>0
\end{array}\right.
$$

W 为权值矩阵，即权重，b 的含义为偏置。权重表示了输入的每个信号的重要程度，而偏置表示了神经元被激活的难易程度。感知机将输入的数据乘以权值之后，加上偏置，若得到的值小于等于0，则输出1，神经元被激活，否则输出0。可见，b 的值越大，则得到的输出值$o_i$ 为1 的可能性就越小，神经元越难被激活。

单层感知机能够将输入特征与权重和偏置进行**线性组合**，并利用线性预测函数实现对输入特征的分类，最终得到了0 或1 的输出，这在机器学习中属于二分类的学习算法，是一种线性分类器。我们将单层感知机的这种处理方式放在输入空间来理解，则感知机算法相当于找到了一个将实例划分为正样本和负样本的分离超平面。

单层感知机只能表示一条直线分割的空间，仅支持线性可分的数据集，而对非线性空间，如“异或”这种非线性运算，单层感知机不具备对其进行划分的能力。

![20230523174539](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230523174539.png)

通过在输入层和输出层之间加入新的层，来实现弯曲折线的划分效果，从而产生了多层感知机(multilayer perceptron，MLP)。它除了输入层和输出层，中间可以有多个称作隐藏层的层。单层感知机无法表示的非线性空间，可以通过增加隐藏层来进行划分。

![20230523111721](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230523111721.png)

### 从多层感知机到神经网络

多层感知机中的权重和偏置参数目前仍然是人为设定的，而为一个多层
感知机确定出合适的、能够达到预期效果的权重和偏置参数是很难掌握的，而且也无法通过感知机的学习规则来进行权重训练。

基于反向传播算法，可以实现使网络在数据正向传播和误差反向传播两个过程中进行学习，从而从输入数据中自动地学习到合适的网络参数(权重和偏置)，突破多层感知机的学习瓶颈，得到具备学习能力的神经网络。

神经网络在网络结构上可以看作是将多个感知机进行了组合，采用不同的方式来连接多个感知机，并采用不同的激活函数进行激活，同时神经网络具备参数的学习能力。

### 激活函数

为什么要使用激活函数呢？

那是因为在神经网络中不使用激活函数的话，则相当于网络中每一层神经元的输入都是上一层神经元输出的线性组合，那么，很容易验证，无论在神经网络中设置了多少隐藏层，最终得到的输出都将是输入特征的线性组合，这样构建出的神经网络与没有隐藏层的单层感知机效果相当，神经网络的拟合能力也非常有限。

而引入非线性函数作为激活函数，就相当于为神经网络中的神经元引入了非线性因素，使得神经网络具备了学习的能力及更强大的拟合能力，
能够更好地拟合任何非线性函数的目标函数，从而将神经网络应用到更多的非线性模型中。激活函数是感知机与神经网络之间的纽带。

常见的激活函数包括：

1. Sigmoid 函数（又称为 Logistic 函数）：

$$
f(x) = \frac{1}{1+e^{-x}}
$$

![sigmod函数](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/sigmod函数.jpg)

在输入值趋近于正无穷或者负无穷时，Sigmoid 函数的导函数值
趋近于0，在反向传播时，Sigmoid 的这一特点会使得梯度更新十分缓慢，容易出现梯度消失的问题，从而无法完成深层网络的训练。Sigmoid 函数不是以0 为中心的函数，得到的输出值均为正，这会大大降低神经网络的收敛速度。

![20230523135539](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230523135539.png)

2. Tanh函数：

Tanh 函数与Sigmoid 函数相似，但Tanh 函数是原点对称的，其值域在−1～1之间:(解决了所有输出值符号相同的问题，能够对输入数据进行中心化，使得数据的均值更接近0，从而使下一层的学习变得简单一点。)

$$
f(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}} = \frac{2}{1+e^{-2x}} -1
$$

但是同样存在反向传播时候，梯度消失以及涉及幂运算，计算成本较高的问题。

![20230523133427](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230523133427.png)

3. ReLU 函数：

ReLU(rectified linear units)是目前设计神经网络使用非常广泛的激活函数，常用于**隐藏层神经元的输出**，表达式如下所示：

$$
f(x) = max(0,x)
$$

![20230523133531](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230523133531.png)

当x≤0 时，ReLU 函数会将输入转换为0，而当x>0 时，ReLU函数的导数不变。

当使用ReLU 作为激活函数，输入为负值时，ReLU 函数会将输入转换为0，因
而对应的神经元将不被激活，也就是说ReLU 作为激活函数进行激活时，在一段时
间内，只有一部分神经元被激活，并不会同时激活所有的神经元，这一特性使得神经网络具有**稀疏性**，更便于计算。当输入为正值时，ReLU 函数能够保持梯度不衰减，不会出现梯度消失问题。另外，ReLU 函数的计算过程非常简单，使用ReLU作为激活函数只需要判断输入是否大于0，运算复杂度低，能够大大提高运算速度，网络的**收敛速度**也远快于Sigmoid 和Tanh。

4. Leaky ReLU 函数：

Leaky ReLU 函数是对ReLU 函数的改进，其表达式如下所示：

$$
f(x) = max(0,x)+\alpha min(0,x)
$$

也即是，

$$
f(x) = \begin{cases} x, & \text{if $x>0$} \\ \alpha x, & \text{otherwise} \end{cases}
$$

该函数相比于ReLU，保留了一些负轴的值，缓解了激活值过小而导致神经元参数无法更新的问题，其中$\alpha$默认0.01。

![20230523134404](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230523134404.png)

5. Softmax 函数：

Softmax 可以将网络输出层中多个神经元得到的输出映射到0～1 之间，实现将
网络的输出转换为每一类别的归一化概率，因此通常用于**多分类神经网络中分类器
的输出层**，如目标检测中的分类器输出层。Softmax 函数的表达式如下所示：

$$
f(x_i) = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}
$$

其中，i表示第i个神经元，n表示输出层的神经元个数。

![20230523134948](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230523134948.png)

这张图更加形象一点点：

![20230523135259](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230523135259.png)

### 如何选择不同的激活函数

应如何选择激活函数呢？

通常Sigmoid、Tanh 和ReLU 都可以用在隐藏层中，但由于Sigmoid 和Tanh 存在梯度消失问题，会尽量避免使用。若要在隐藏层使用Sigmoid 或Tanh，由于Tanh 是以0 为中心的，通常性能会比Sigmoid函数要好。

ReLU 函数是一个比较通用的激活函数，在大多数的网络训练中都可以使用，但要注意ReLU 只能用于隐藏层中。

在分类器中，特别是二分类的情况，可以将Sigmoid 函数用于网络的输出层。在多分类问题中，可以将Softmax 函数用于网络的输出层。事实上，采用不同激活函数训练得到的网络性能好坏并没有一个统一的定论，一般来讲我们可以先采用ReLU 函数，若未能达到好的训练效果，再尝试采用其他激活函数。
