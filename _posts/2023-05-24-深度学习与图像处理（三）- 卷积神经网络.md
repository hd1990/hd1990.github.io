---
layout: post
title: 深度学习与图像处理（三）- 卷积神经网络
date: 2023-05-24
description: "深度学习与图像处理（三）- 卷积神经网络"
tag: 机器学习
---
## 卷积神经网络

Convolutional Neural Network卷积神经网络被广泛用于图像的识别，语音的处理等等，与之前的神经网络相比，出现了卷积层和池化层。

### 卷积层

CNN中，有时将卷积层的输入输出数据称为特征图（feature map）。其中，卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）。

![20230525100908](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230525100908.png)

如图所示，卷积运算相当于对输入数据应用滤波器，输入大小是(4,4)，输出大小是(2,2)，滤波器大小是(3,3)。对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用。如下图所示，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为乘积累加运算）。然后，将这个结果保存到输出的对应位置。

![20230525101131](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230525101131.png)

#### 填充

在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（比
如0 等），这称为填充（padding）。

使用填充主要是为了调整输出的大小。比如，对大小为(4, 4) 的输入数据应用(3, 3) 的滤波器时，输出大小变为(2, 2)，相当于输出大小比输入大小缩小了2 个元素。这在反复进行多次卷积运算的深度网络中会成为问题。

为了避免出现这样的情况，就要使用填充。在刚才的例子中，将填充的幅度设为1，那么相对于输入大小(4, 4)，输出大小也保持为原来的(4, 4)，卷积运算可以在保持空间大小不变的情况下将数据传递给下一层。

![20230525102624](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230525102624.png)

#### 步幅

应用滤波器的位置间隔称为步幅（stride）。之前的例子中步幅都是1。以下是步幅为2的卷积运算的例子：

![20230525102931](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230525102931.png)

#### 卷积前后的大小

假设输入前的大小为(H,W)，滤波器大小为(FH, FW)，输出大小为(OH,OW)，填充为P，步幅为S。此时，输出大小可通过下面的算式进行计算。

$$
\begin{matrix}OH= \frac{H+2P-FH}{S}+1 \\ OW= \frac{W+2P-FW}{S}+1 \\ \end{matrix}
$$

图像除了长高方向之外，还需要处理通道方向。一般来说，先进行卷积计算，然后再把三个通道的计算结果相加，从而得到输出。

![20230525103528](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230525103528.png)

如果要在通道方向上也拥有多个卷积运算的输出，比如说有FN个滤波器，通过应用FN个滤波器，输出特征图也有FN个。作为4 维数据，滤波器的权重数据要按(output_channel, input_channel, height, width) 的顺序书写。比如，通道数为3、大小为5 × 5 的滤波器有20 个时，可以写成(20, 3, 5, 5)。

![20230525103737](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230525103737.png)

我们希望卷积运算也同样对应批处理，比如说如下图所示，对N个数据进行批处理时，数据的输入输出形状如图所示。

![20230525104123](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230525104123.png)

### 池化层

池化是缩小高、长方向上的空间的运算。比如，如下图所示，进行将2 × 2 的区域集约成1 个元素的处理，取这个2 × 2 区域的最大值被称为Max池化，可以缩小空间大小。

![20230525104314](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230525104314.png)

在这个例子中，步幅设置为2，所以2 × 2窗口的移动间隔为2个元素。

池化计算不会改变输入数据的通道数，是按照通道独立进行的。

![20230525104544](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230525104544.png)

### 全连接层

比如说输入图像是一通道，高28像素，宽28像素，经过全连接层之后被排列成一列，以784个数据的形式。

### CNN在看什么？

局部特征，如边缘或斑块等原始信息，则随着层次加深，提取的信息也愈加复杂、抽象。

#### CNN的实现

组合卷积层和池化层，可以构建出CNN。CNN的结构如下图所示。

![20230525105727](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230525105727.png)

## 经典CNN

### LeNet

它有连续的卷积层和池化层（正确地讲，是只“抽选元素”的子采样层），最
后经全连接层输出结果。

![20230525110448](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230525110448.png)

和“现在的CNN”相比，LeNet 有几个不同点。第一个不同点在于激活函数。LeNet 中使用sigmoid 函数，而现在的CNN中主要使用ReLU函数。

此外，原始的LeNet 中使用子采样（subsampling）缩小中间数据的大小，而
现在的CNN中Max池化是主流。

### AlexNet

它的网络结构和LeNet基本上没有什么不同，但是激活函数使用ReLU；使用进行局部正规化的LRN（Local Response Normalization）层；使用Dropout。

![20230525110632](https://cdn.jsdelivr.net/gh/ChanJeunlam/PicgoBed/blogs/pictures/20230525110632.png)
